from scipy import stats
from fitter import Fitter
from scipy import *
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KernelDensity
from sklearn.model_selection import GridSearchCV
from iteration_utilities import deepflatten
import kalepy as kale

global best_kde 

def abstraction(output2):
    output2 = list(deepflatten(output2, depth=1))
    params = {}
    list_of_dists = ['alpha', 'anglit', 'arcsine', 'argus','beta','betaprime','bradford',
                    'burr','burr12','cauchy','chi','chi2','cosine','crystalball','dgamma',
                    'dweibull','erlang','expon','exponnorm','exponweib','exponpow','f','fatiguelife',
                    'fisk','foldcauchy','foldnorm','genlogistic','gennorm','genpareto','genexpon',
                    'genextreme','gausshyper','gamma','gengamma','genhalflogistic','genhyperbolic',
                    'geninvgauss','gilbrat','gompertz','gumbel_r','gumbel_l','halfcauchy','halflogistic',
                    'halfnorm','halfgennorm','hypsecant','invgamma','invgauss','invweibull','johnsonsb',
                    'johnsonsu','kappa4','kappa3','ksone','laplace',
                    'laplace_asymmetric','levy','levy_l','logistic','loggamma',
                    'loglaplace','lognorm','loguniform','lomax','maxwell','mielke','moyal','nakagami',
                    'ncx2','ncf','nct','norm','norminvgauss','pareto','pearson3','powerlaw','powerlognorm',
                    'powernorm','rdist','rayleigh','rice','recipinvgauss','semicircular','skewcauchy',
                    'skewnorm','t','trapezoid','triang','truncexpon','truncnorm',
                    'tukeylambda','uniform','vonmises','vonmises_line','wald','weibull_min','weibull_max',
                    'wrapcauchy']
    

    results = []
    for i in list_of_dists:
        #print(i)
        dist = getattr(stats, i)
        param = dist.fit(output2)
        a = stats.kstest(output2, i, args=param)
        results.append((i, a[0], a[1]))
        params[i] = param

    results.sort(key=lambda x:float(x[2]), reverse=True)
    i = 0
    print("Best distribution based on p-value")
    for j in results:
        print("{}:,statistic={}, pvalue={}".format(j[0], j[1], round(j[2],3)))
        if i < 1:
            dist = getattr(stats, j[0])
            p_value = j[2]
            round_param = [round(num, 2) for num in param]
            print("Parameters for the best distribution are:", round_param[:])
            i += 1
    print("*"*100)
    print("*"*100)
    print("Now showing results based on SSE")

    f = Fitter(output2, distributions= list_of_dists)
    f.fit()
    print("The best distribution & its parameters")
    best_dist = f.get_best(method = 'sumsquare_error')
    print(best_dist)
    print("Best five distributions based on SSE")
    print(f.summary())
    if p_value < 0.05:
        print('\033[1m' + "None of the distributions fit the data, try non-parametric 'np_abstraction' method"+'\033[0m')
    else:
        print("Replace the code block by:")
        p1 = f.get_best(method = 'sumsquare_error')
        p = list(p1.keys())
        #round_t = [round(num, 2) for num in f.fitted_param[(p[0])]]
        print("yield self.hold(stats.rvs.{}{})".format(p[0], f.fitted_param[(p[0])]))
       
            
        

def my_scores(estimator, X):
    scores = estimator.score_samples(X)
    # Remove -inf
    scores = scores[scores != float('-inf')]
    # Return the mean values
    return np.mean(scores)

def np_abstraction(data):
    global best_kde
        
    data = list(deepflatten(data, depth=1))
    x = np.array(data)  
    x_train = x[:, np.newaxis]
    x_test = np.linspace(np.min(x), np.max(x), 2000)[:, np.newaxis]
    kernels = ['gaussian', 'tophat']
    # bandwith values
    h_vals = np.arange(0.2, 1, .2)
    grid = GridSearchCV(KernelDensity(),{'bandwidth': h_vals, 'kernel': kernels},scoring=my_scores)
    grid.fit(x_train)
    best_kde = grid.best_estimator_
    log_dens = best_kde.score_samples(x_test)
    plt.fill(x_test, np.exp(log_dens), c='green')
    plt.title("Best Kernel: " + best_kde.kernel+" h="+"{:.2f}".format(best_kde.bandwidth))
    plt.show()
    print ("Use the following in place of your code" + " " + "rv = best_kde.sample()[0][0]")
    return best_kde

def np_abstraction_kale(data):        
    data = list(deepflatten(data, depth=1))
    edge = min(data)  # edge is used to set the lower limit below which the dat can not go
    # Histogram the data, use fixed bin-positions

    plt.hist(data, bins=15, density=True, alpha=1, label='data', color='w', edgecolor='k')
    # the point corrosponds to data points (x-axis) at which the probability density is given
    points, pdf = kale.density(data, probability=True)
    plt.plot(points, pdf, 'k--', lw=1.0, alpha=1, label='Reflecting KDE')
    kde = kale.KDE(data)
    plt.legend()
    plt.show()
   
    return points, pdf, kde
            
            
# the below method is to be used when the lower bound is fixed
def np_abstraction_lb(data):        
    data = list(deepflatten(data, depth=1))
    edge = min(data)  # edge is used to set the lower limit below which the dat can not go
    # Histogram the data, use fixed bin-positions

    plt.hist(data, bins=15, density=True, alpha=1, label='data', color='w', edgecolor='k')
    # the point corrosponds to data points (x-axis) at which the probability density is given
    points, pdf = kale.density(data, reflect=[edge, None], probability=True)
    plt.plot(points, pdf, 'k--', lw=1.0, alpha=1, label='Reflecting KDE')
    kde = kale.KDE(data)
    
    plt.legend()
    plt.show()
    
    return points, pdf, kd
